# -*- coding: utf-8 -*-
"""paws_tf_hs_Houston13_github_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qQNV46NQHJV-IXopawlewm8DNFLJbQKP
"""

## Install tensorflow-addons before running
##!pip install -q tensorflow_addons

# Import Relevant libraries and classes
import scipy.io as sio
import numpy as np
import tqdm
from sklearn.decomposition import PCA
import tensorflow as tf
keras = tf.keras
from keras import backend as K
from keras import regularizers
from keras.models import Model, Sequential
from keras.layers import Dense, Input
from keras.layers import Conv2D, Flatten, Lambda, Conv3D, Conv3DTranspose,BatchNormalization,Conv1D, GlobalAveragePooling2D
from keras.layers import Reshape, Conv2DTranspose, Concatenate, Multiply, Add, MaxPooling3D, Dropout, MaxPooling2D, DepthwiseConv2D
from keras import Model
from keras.datasets import mnist
from keras.losses import mse, binary_crossentropy
from keras import backend as K
from sklearn.metrics import confusion_matrix

#from models import wide_resnet
import matplotlib.pyplot as plt
import tensorflow as tf
import time
import numpy as np
import tqdm

from copy import deepcopy

def get_paws_loss(tau=0.1, T=0.25, me_max=True):
    """
    Computes PAWS loss

    :param multicrop: number of small views
    :param tau: cosine temperature
    :param T: sharpening temperature
    :param me_max: mean entropy maximization flag
    :return: PAWS loss
    """

    def sharpen(proba):
        """ Target sharpening function """
        sharp_p = proba ** (1.0 / T)
        sharp_p /= tf.reduce_sum(sharp_p, axis=1, keepdims=True)
        return sharp_p

    def snn(query, supports, labels):
        """ Soft Nearest Neighbours similarity classifier """
        # Step 1: Normalize embeddings
        query = tf.math.l2_normalize(query, axis=1)
        supports = tf.math.l2_normalize(supports, axis=1)

        # Step 2: Compute similarity
        return tf.nn.softmax(query @ tf.transpose(supports) / tau, axis=1) @ labels

    def loss(
        anchor_views,
        anchor_supports,
        anchor_support_labels,
        target_views,
        target_supports,
        target_support_labels,
        sharpen=sharpen,
        snn=snn,
    ):

        # Step 1: Compute anchor predictions
        probs = snn(anchor_views, anchor_supports, anchor_support_labels)

        # Step 2: Compute targets for anchor predictions
        targets = tf.stop_gradient(snn(target_views, target_supports, target_support_labels))
        targets = tf.stop_gradient(sharpen(targets))

        # For numerical stability
        mask = tf.stop_gradient(tf.math.greater(targets, 1e-4))
        mask = tf.stop_gradient(tf.cast(mask, dtype=targets.dtype))
        targets *= tf.stop_gradient(mask)

        # Step 3: compute cross-entropy loss H(targets, queries)
        loss = tf.reduce_mean(tf.reduce_sum(tf.math.log(probs ** (-targets)), axis=1))

        # Step 4: compute me-max regularizer
        rloss = 0.0
        if me_max:
            avg_probs = tf.reduce_mean(sharpen(probs), axis=0)
            rloss -= tf.reduce_sum(tf.math.log(avg_probs ** (-avg_probs)))

        return loss, rloss

    return loss

import tensorflow_addons as tfa
import tensorflow as tf
import numpy as np

SIZE_CROPS = [9, 7]  # 32: global views, 18: local views
NUM_CROPS = [2, 6]
GLOBAL_SCALE = [0.75, 1.0]
LOCAL_SCALE = [0.3, 0.75]
AUTO = tf.data.AUTOTUNE

#@tf.function
def random_band_average(x):
  _,_,b = np.shape(x)
  ind1 = np.sort(np.random.choice(np.arange(10,b-10,20),replace=False, size = 3))
  list1 = []
  j = 0
  for i in range(b):
    in2 = i+9*j
    if j<=2:
      if in2==ind1[j]:
        mean1 = tf.math.reduce_mean(x[:,:,in2:in2+10], axis = 2)
        j = j+1
        for k in range(10):
          list1.append(mean1)

      else:
        if in2<b:
          list1.append(x[:,:,in2])

    else:
        if in2<b:
          list1.append(x[:,:,in2])

  return tf.cast(tf.transpose(tf.stack(list1), [1,2,0]), 'uint8')

@tf.function
def random_band_drop(x):
  _,_,b = np.shape(x)
  x1 = np.ones(np.shape(x), dtype = 'uint8')
  ind1 = np.random.choice(np.arange(b),replace=False, size = (int(0.2*b)))
  x1[:,:,ind1] = 0
  x = x*x1
  return tf.cast(x, 'uint8')

@tf.function
def random_pixel_removal(x):

  mask1 = np.ones(np.shape(x), dtype = 'uint8')
  w,h,_ = np.shape(x)
  k = int(0.2*w*h)
  for i in range(k):
    ind1 = np.random.choice(np.arange(h),replace=False, size = (2))
    mask1[ind1[0], ind1[1],:] = 0
  x = x*mask1
  return tf.cast(x, 'uint8')

@tf.function
def random_rotation(x):
  import math
  ang1 = np.random.choice([0,math.pi/2,math.pi,3*math.pi/2])
  x = tf.cast(tfa.image.rotate(x, ang1), 'uint8')
  return x

@tf.function
def random_flip_lr(x):
  return tf.cast(tf.image.random_flip_left_right(x), 'uint8')

@tf.function
def random_flip_ud(x):
  return tf.cast(tf.image.random_flip_up_down(x), 'uint8')

@tf.function
def img_eq(x):
  return tf.cast(tfa.image.equalize(x.astype('uint8')), 'uint8')

@tf.function
def random_resize_crop_img(x):
  shp = tf.shape(x)
  return tf.cast(tf.image.resize(tf.image.random_crop(x, [6, 6, shp[2]]), [shp[0],shp[1]]), 'uint8')

@tf.function
def float_parameter(level, maxval):
    """
    """
    return tf.cast(level * maxval / 10.0, tf.float32)


@tf.function
def sample_level(n):
    """
    Uniform sampling with [0.1, n) range.
    """
    return tf.random.uniform(shape=[1], minval=0.1, maxval=n, dtype=tf.float32)


@tf.function
def random_apply(func, x, p):
    """
    Utility to apply a function to a given input with a probability.
    """
    if tf.random.uniform([], minval=0, maxval=1) < p:
        return func(x)
    else:
        return x

@tf.function
def random_resize_distort_crop(image, a, b):

    # Flip and color distortions
    image = random_apply(random_resize_crop_img, image, p=0.6)
    #image = random_apply(img_eq, image, p=0.8)
    image = random_apply(random_flip_ud, image, p=0.75)
    image = random_apply(random_flip_lr, image, p=0.75)
    image = random_apply(random_rotation,  image, p=0.75)
    image = random_apply(random_band_average,  image, p=0.75)
    image = random_apply(random_band_drop, image, p=0.75)
    image = random_apply(random_pixel_removal, image, p=0.75)
    return image


def get_multicrop_loader(ds: tf.data.Dataset):
    """
    Returns a multi-crop dataset.

    :param ds: a TensorFlow dataset object (containing only unlabeled images)
    :return: a multi-crop dataset
    """
    loaders = tuple()
    for i, num_crop in enumerate(NUM_CROPS):
        for _ in range(num_crop):
            if SIZE_CROPS[i] == 9:
                scale = GLOBAL_SCALE
            elif SIZE_CROPS[i] == 7:
                scale = LOCAL_SCALE

            loader = ds.map(
                lambda x: random_resize_distort_crop(x, scale, SIZE_CROPS[i]),
                num_parallel_calls=AUTO,
                deterministic=True,
            )
            loaders += (loader,)

    return tf.data.Dataset.zip(loaders)

import tensorflow as tf
import numpy as np
import os

GLOBAL_SCALE = [0.75, 1.0]
AUTO = tf.data.AUTOTUNE

def onehot_encode(labels, label_smoothing=0.1):
    """
    One-hot encode labels with label smoothing.

    :param labels: (batch_size, )
    return: one-hot encoded labels with optional label smoothing
    """
    labels = tf.one_hot(labels, depth=np.size(np.unique(Y_TRAIN)))
    # Reference: https://t.ly/CSYO)
    labels *= 1.0 - label_smoothing
    labels += label_smoothing / labels.shape[1]
    return labels

def sample_dataset():
    """
    Returns a randomly sampled dataset and saves the randomly sampled
    indices.
    """
    '''if not os.path.exists(config.SUPPORT_IDX):
        sampled_idx = np.random.choice(len(X_TRAIN), config.SUPPORT_SAMPLES)
        np.save(config.SUPPORT_IDX, sampled_idx)
    else:
        sampled_idx = np.load(config.SUPPORT_IDX)'''

    sampled_train, sampled_labels = X_TRAIN, Y_TRAIN.squeeze()
    return tf.data.Dataset.from_tensor_slices((sampled_train, sampled_labels))


def dataset_for_class(i):
    """
    Returns a dataset containing filtered with given class label.

    :param ds: TensorFlow Dataset object
    :param i: class label
    :return: filtered dataset
    """
    ds = sample_dataset()
    i = tf.cast(i, tf.uint8)
    return ds.filter(lambda image, label: label == i).repeat()


def get_support_ds(bs, aug=True):
    """
    Prepares TensorFlow dataset with sampling as suggested in:
    https://arxiv.org/abs/2104.13963 (See Appendix C)

    :param bs: batch size (int)
    :param aug: boolean indicating if augmentation should be applied
    :return: a multi-crop dataset
    """
    # As per Appendix C, for CIFAR10 2x views are needed for making
    # the network better at instance discrimination.
    # Reference:
    # https://stackoverflow.com/questions/46938530/
    ds = tf.data.Dataset.range(np.size(np.unique(Y_TRAIN))).interleave(
        dataset_for_class,
        cycle_length=np.size(np.unique(Y_TRAIN)),
        num_parallel_calls=AUTO,
        deterministic=True,
    )
    SUP_VIEWS = 2
    # As per Appendix C, for CIFAR10 2x views are needed for making
    # the network better at instance discrimination.
    loaders = tuple()
    for _ in range(SUP_VIEWS):
        if aug:
            loader = ds.map(
                lambda x, y: (
                    random_resize_distort_crop(x, GLOBAL_SCALE, 9),
                    y,
                ),
                num_parallel_calls=AUTO,
                deterministic=True,
            )
        else:
            loader = ds
        loaders += (loader,)

    final_ds = tf.data.Dataset.zip(loaders)
    return final_ds.batch(bs).prefetch(AUTO)

## Read the annotated samples for training (guiding)
## There are total 1500 samples for Houston 2013 dataset.

X_TRAIN = np.load('/content/gdrive/MyDrive/paws_tf_hsi/Houston13/train_patches_hs.npy')
Y_TRAIN = np.load('/content/gdrive/MyDrive/paws_tf_hsi/Houston13/train_labels.npy').astype('uint8')-1

support_ds1 = get_support_ds(1500) #config.SUPPORT_BS = 1500
support_ds2 = get_support_ds(1500) #config.SUPPORT_BS = 1500
print("Data loaders prepared.")

## Create dataloaders for the unlabelled samples. The batch  size is 113

MULTICROP_BS = 113

x_train1 = np.load('patches1.npy')
train_ds1 = tf.data.Dataset.from_tensor_slices(x_train1)
multicrop_ds1 = get_multicrop_loader(train_ds1)
multicrop_ds1 = (
    multicrop_ds1.shuffle(MULTICROP_BS*10)
    .batch(MULTICROP_BS)
    .prefetch(AUTO)
)# config.MULTICROP_BS = 113
del x_train1

x_train2 = np.load('patches2.npy')
train_ds2 = tf.data.Dataset.from_tensor_slices(x_train2)
multicrop_ds2 = get_multicrop_loader(train_ds2)
multicrop_ds2 = (
    multicrop_ds2.shuffle(MULTICROP_BS*10)
    .batch(MULTICROP_BS)
    .prefetch(AUTO)
)# config.MULTICROP_BS = 113
AUTO = tf.data.AUTOTUNE
STEPS_PER_EPOCH = int(len(x_train2) // MULTICROP_BS)
WARMUP_EPOCHS = 10
WARMUP_STEPS = int(WARMUP_EPOCHS * STEPS_PER_EPOCH)
del x_train2

multicrop_ds = tf.data.Dataset.zip((multicrop_ds1, multicrop_ds2))
#del multicrop_ds1, multicrop_ds2
del train_ds1, train_ds2

## Design the feature Extractor. This would be used for pre-training

def conv_layer(x, f):

    convp = Conv2D(f, (1,1), strides=(1, 1), padding='same', dilation_rate=(1, 1),
                            activation=None, kernel_initializer='glorot_uniform',
                            kernel_regularizer=regularizers.l2(0.01))(x)

    conv1 = DepthwiseConv2D((3,3), padding='same', strides=(1, 1), dilation_rate = 1,
                            activation=None, kernel_initializer='glorot_uniform',
                            kernel_regularizer=regularizers.l2(0.01))(convp)

    conv1 = BatchNormalization()(conv1)
    conv1 = keras.activations.relu(conv1)

    return conv1

def conv_2d(x,f):

    conv1 = conv_layer(x,f)
    conv1 = BatchNormalization()(conv1)
    conv1 = keras.activations.relu(conv1)

    conv2 = conv_layer(conv1,f)
    conv2 = BatchNormalization()(conv2)
    conv2 = keras.activations.relu(conv2)

    cat1 = Concatenate(axis = 3)([conv1, conv2])

    mp1 = MaxPooling2D(pool_size=(2, 2), padding="valid")(cat1)

    return mp1

def blk(x):

  x0 = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255)(x)
  x1 = Reshape([9,9,12,12])(x0)
  c0 = Conv3D(12, (3,3,4), padding = 'same')(x1)
  ca = Add()([x1,c0])
  cr = Reshape([9,9,144])(ca)
  c1 = conv_2d(cr,32)
  c2 = conv_2d(c1,64)
  c3 = conv_2d(c2,128)
  g1 = GlobalAveragePooling2D()(c3)
  fc1 = Dense(128, activation = 'relu')(g1)
  #fc2 = Dense(15, activation = 'softmax')(fc1)

  return fc1

def get_network():

  x = Input((9,9,144))
  clsf = blk(x)
  model = Model(x, clsf, name = 'att_clf')
  return model

## intitalize the Encoder and the Optimizer
encoder_model = get_network()
optimizer = keras.optimizers.Adam(0.5)
encoder_model.summary()

## Define the training step
import tensorflow as tf

paws_loss = get_paws_loss(tau=0.1, T=0.25, me_max=True)

def train_step1(unsup_images, sup_loader1, sup_loader2, encoder: tf.keras.Model):
    """
    One step of PAWS training.

    :param unsup_images: unsupervised images (nb_crops, batch_size, h, w, nb_channels)
    :param sup_loader: tuple of images and one-hot encoded labels from the support set
    :param encoder: trunk with projection head
    :return: losses (ce and me-max) and gradients
    """
    unsup_images1, unsup_images2 = unsup_images[0], unsup_images[1]
    # Get batch size for unsupervised images
    u_batch_size = tf.shape(unsup_images1[0])[0]

    # Unsupervised global (2) and local (6) views
    imgs1 = tf.concat([u for u in unsup_images1[:2]], axis=0)
    mc_imgs1 = tf.concat([u for u in unsup_images1[2:]], axis=0)

    imgs2 = tf.concat([u for u in unsup_images2[:2]], axis=0)
    mc_imgs2 = tf.concat([u for u in unsup_images2[2:]], axis=0)

    # Segregate images and labels from support set
    simgs1, labels1 = sup_loader1
    simgs2, labels2 = sup_loader2

    # Concatenate unlabeled images and labeled support images
    imgs1, simgs1 = tf.cast(imgs1, tf.float32), tf.cast(simgs1, tf.float32)
    imgs1 = tf.concat([imgs1, simgs1], axis=0)

    imgs2, simgs2 = tf.cast(imgs2, tf.float32), tf.cast(simgs2, tf.float32)
    imgs2 = tf.concat([imgs2, simgs2], axis=0)

    with tf.GradientTape() as tape:
        # Pass through the global views (including images from the
        # support set) and multicrop views.
        # h: trunk output, z, z_mc: projection output
        z1 = encoder(imgs1, training=True)
        z_mc1 = encoder(mc_imgs1, training=True)

        z2 = encoder(imgs2, training=True)
        z_mc2 = encoder(mc_imgs2, training=True)

        # Determine anchor views / supports and their  corresponding
        # target views/supports (we are not using prediction head)
        h1 = z1
        h2 = z2

        target_supports1 = h1[2 * u_batch_size :]
        target_views1 = h1[: 2 * u_batch_size]
        target_views1 = tf.concat(
            [target_views1[u_batch_size:], target_views1[:u_batch_size]], axis=0
        )
        anchor_supports1 = z1[2 * u_batch_size :]
        anchor_views1 = z1[: 2 * u_batch_size]
        anchor_views1 = tf.concat([anchor_views1, z_mc1], axis=0)

        target_supports2 = h2[2 * u_batch_size :]
        target_views2 = h2[: 2 * u_batch_size]
        target_views2 = tf.concat(
            [target_views2[u_batch_size:], target_views2[:u_batch_size]], axis=0
        )
        anchor_supports2 = z2[2 * u_batch_size :]
        anchor_views2 = z2[: 2 * u_batch_size]
        anchor_views2 = tf.concat([anchor_views2, z_mc2], axis=0)

        # Compute paws loss with me-max regularization
        (ploss1, me_max1) = get_paws_loss(tau=0.1, T=0.25, me_max=True)(
            anchor_views=anchor_views1,
            anchor_supports=anchor_supports1,
            anchor_support_labels=labels1,
            target_views=target_views1,
            target_supports=target_supports1,
            target_support_labels=labels1,
        )

        (ploss2, me_max2) = get_paws_loss(tau=0.1, T=0.25, me_max=True)(
            anchor_views=anchor_views2,
            anchor_supports=anchor_supports2,
            anchor_support_labels=labels2,
            target_views=target_views2,
            target_supports=target_supports2,
            target_support_labels=labels2,
        )

        loss = ploss1+ploss2+me_max1+me_max2

        # Compute gradients
    gradients = tape.gradient(loss, encoder.trainable_variables)
    return ploss1+ploss2, me_max1+me_max2, gradients

epoch_ce_losses = []
epoch_me_losses = []

# Commented out IPython magic to ensure Python compatibility.
encoder_model.save('paws/model/model')
# E is the number of epochs for pretraining the model
for e in range(50):
    print(f"=======Starting epoch: {e}=======")
    start_time = time.time()
    epoch_ce_loss_avg = tf.keras.metrics.Mean()
    epoch_me_loss_avg = tf.keras.metrics.Mean()

    for i, unsup_imgs in enumerate(multicrop_ds):
        # Sample support images, concat the images and labels, and
        # then apply label-smoothing.
        global iter_supervised1, iter_supervised2
        try:
            support_images_one1, support_images_two1 = next(iter_supervised1)
            support_images_one2, support_images_two2 = next(iter_supervised2)
        except Exception:
            iter_supervised1 = iter(support_ds1)
            iter_supervised2 = iter(support_ds2)
            support_images_one1, support_images_two1 = next(iter_supervised1)
            support_images_one2, support_images_two2 = next(iter_supervised2)

        ## For set 1
        support_images1 = tf.concat(
            [support_images_one1[0], support_images_two1[0]], axis=0
        )
        support_labels1 = tf.concat(
            [support_images_one1[1], support_images_two1[1]], axis=0
        )

        # Label smoothing = 0.1
        support_labels1 = onehot_encode(
            support_labels1, 0.1
        )

        ## For set 2
        support_images2 = tf.concat(
            [support_images_one2[0], support_images_two2[0]], axis=0
        )
        support_labels2 = tf.concat(
            [support_images_one2[1], support_images_two2[1]], axis=0
        )

        support_labels2 = onehot_encode(
            support_labels2, 0.1
        )

        # Perform training step
        batch_ce_loss, batch_me_loss, gradients = train_step1(
            unsup_imgs, (support_images1, support_labels1),  (support_images2, support_labels2), encoder_model
        )

        # Update the parameters of the encoder
        optimizer.apply_gradients(zip(gradients, encoder_model.trainable_variables))

        if (i % 50) == 0:
            print(
                "[%d, %5d] loss: %.3f (%.3f %.3f)"
#                 % (
                    e,
                    i,
                    batch_ce_loss.numpy() + batch_me_loss.numpy(),
                    batch_ce_loss.numpy(),
                    batch_me_loss.numpy(),
                )
            )
        epoch_ce_loss_avg.update_state(batch_ce_loss)
        epoch_me_loss_avg.update_state(batch_me_loss)

    print(
        f"Epoch: {e} CE Loss: {epoch_ce_loss_avg.result():.3f}"
        f" ME-MAX Loss: {epoch_me_loss_avg.result():.3f}"
        f" Time elapsed: {time.time()-start_time:.2f} secs"
    )
    print("")
    epoch_ce_losses.append(epoch_ce_loss_avg.result())
    epoch_me_losses.append(epoch_me_loss_avg.result())
    np.save('paws/model/ce_loss',epoch_ce_losses)
    np.save('paws/model/me_loss',epoch_me_losses)

    if e%5==0:
      encoder_model.save('paws/model/model_temp')

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
plt.plot(epoch_ce_losses)
plt.title("PAWS Training Cross-Entropy Loss", fontsize=12)
plt.grid()

"""# Fine Tuning"""

## Load train and test sets
X_TRAIN = np.load('train_patches_hs.npy')
Y_TRAIN = np.load('train_labels.npy').astype('uint8')-1

X_TEST = np.load('test_patches_hs.npy')
Y_TEST = np.load('test_labels.npy').astype('uint8')-1

model = tf.keras.models.load_model('paws/model/model')

trainable = True
for layer in model.layers:
  layer.trainable = trainable

# Create backbone
backbone = tf.keras.Model(
    model.input, model.layers[49].output
)

# We then create our linear classifier and train it.
inputs = tf.keras.layers.Input((None, None, 144))
x = backbone(inputs)

backbone.trainable = trainable
outputs = tf.keras.layers.Dense(15, activation="softmax")(x)
linear_model = tf.keras.Model(inputs, outputs, name="linear_model")
linear_model.summary()

# Function to perform one hot encoding of the class labels

def my_ohc(lab_arr):
    lab_arr_unique =  np.unique(lab_arr)
    r,c = lab_arr.shape
    r_u  = lab_arr_unique.shape

    one_hot_enc = np.zeros((r,r_u[0]), dtype = 'float')

    for i in range(r):
        for j in range(r_u[0]):
            if lab_arr[i,0] == lab_arr_unique[j]:
                one_hot_enc[i,j] = 1

    return one_hot_enc

# Cosine decay for linear evaluation.
cosine_decayed_lr = tf.keras.experimental.CosineDecay(initial_learning_rate=0.03, decay_steps=100)

# Compile model and start training.
linear_model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=tf.keras.optimizers.SGD(0.002, momentum = 0.9))
history = linear_model.fit(x=X_TRAIN, y=my_ohc(np.expand_dims(Y_TRAIN, axis = 1)),
                           validation_data=(X_TEST, my_ohc(np.expand_dims(Y_TEST, axis = 1))), epochs=100)

np.max(history.history['val_accuracy'])

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("Fine tuning loss", fontsize=12)
plt.grid()

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title("Fine tuning accuracy", fontsize=12)
plt.grid()